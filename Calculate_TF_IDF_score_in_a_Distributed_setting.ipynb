{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sdkchris/Projects/blob/main/Calculate_TF_IDF_score_in_a_Distributed_setting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewx6A5AaAcMa"
      },
      "source": [
        "# Calculate TF-IDF score in a Distributed setting\n",
        "\n",
        "**Definition:**\n",
        "\n",
        "TF-IDF or term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. [Wikipedia]\n",
        "\n",
        "It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. tf–idf is one of the most popular term-weighting schemes today. \n",
        "\n",
        "**Methodology:**\n",
        "\n",
        "We will use the concept of parrallel distributed sustems with MPI or Message Passing Interface, which is a standardized message-passing library interface specification. MPI is a very abstract description on how messages can be exchanged between different processes. \n",
        "\n",
        "codingame.com claims that MPI is good for parallelism and high performance computing. In a very straightforward approach you can parallelise a code to do SIMD parallelism : Single Instruction, Multiple Data. That means all of your process will be applying the same treatment on a big pool of data that will be distributed among them. But since MPI does not force you to launch only one program, it is also very convenient when trying to do MIMD parallelism : Multiple Instruction, Multiple Data.\n",
        "\n",
        "  >Master-Worker architecture\n",
        "\n",
        "**Business applications:**\n",
        "* Sentiment analysis,\n",
        "* Information retrieval, \n",
        "* Text summarization,\n",
        "* keyword extraction,\n",
        "* etc. \n",
        "\n",
        "**Data Used:**\n",
        "* fetch_20newsgroups form sklearn datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2-iS0E0AcMg"
      },
      "source": [
        "## Step 1: Data cleaning and text tokenization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " !pip3 install mpi4py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrtPrDQ1BHYd",
        "outputId": "b77234d4-c1ae-4301-cf03-252e0e66cc3c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mpi4py\n",
            "  Downloading mpi4py-3.1.3.tar.gz (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 4.6 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: mpi4py\n",
            "  Building wheel for mpi4py (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-3.1.3-cp37-cp37m-linux_x86_64.whl size=2185301 sha256=ea4a95f830dcb7d3376d5ff9a7c7754398f03018f5cfb6fc5838bf315e9dfc92\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/07/14/6a0c63fa2c6e473c6edc40985b7d89f05c61ff25ee7f0ad9ac\n",
            "Successfully built mpi4py\n",
            "Installing collected packages: mpi4py\n",
            "Successfully installed mpi4py-3.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrAVrpYgAcMh",
        "outputId": "ad8c0948-2c66-4299-a41b-babe4e8ea056"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Processing time: 57.79683037299998\n",
            "\n",
            "The Number of tokenized documents:11314\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from mpi4py import MPI\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "#we will work with Poter Stemmer for word stems \n",
        "poter_stemmer = nltk.PorterStemmer()\n",
        "\n",
        "#mpi routines\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.rank\n",
        "size = comm.size\n",
        "master = 0\n",
        "\n",
        "#text processing\n",
        "tokens  = []\n",
        "\n",
        "#replace unwated characters with an empty space and convert text to lower char\n",
        "def text_process(text):\n",
        "    #remove unwanted characters\n",
        "    text = re.sub('[^a-zA-A]', ' ', text)\n",
        "    text = text.lower()\n",
        "    return(text)\n",
        "\n",
        "#tokenize cleaned text and remove stopwords    \n",
        "def txt_tokenize(text):\n",
        "    #tokenize works by calling word_tokenize\n",
        "    words = word_tokenize(text)\n",
        "    #remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if not word in stop_words]\n",
        "    #capture only stems\n",
        "    words = [poter_stemmer.stem(word) for word in words]\n",
        "    return (words)\n",
        "    \n",
        "#set starting time\n",
        "start_time = MPI.Wtime()\n",
        "\n",
        "if rank == 0:\n",
        "    #get dataset\n",
        "    data_set = fetch_20newsgroups(subset='train', remove=('headers','footers','quotes'))\n",
        "    #convert it into a diction format\n",
        "    data_set = dict(data_set)\n",
        "    #get number of documents\n",
        "    data_array = data_set['data']\n",
        "    \n",
        "    #get number of slices to be sent to works\n",
        "    d_slices = np.array_split(data_array,size)\n",
        "    \n",
        "    #send slice to workers \n",
        "    for worker in range (size-1): #for every worker except master\n",
        "        comm.send(d_slices[worker+1],dest=worker+1)\n",
        "    \n",
        "    \n",
        "\n",
        "else: #for every other worker who is not a master, do receive data\n",
        "    \n",
        "    \n",
        "    worker_data = comm.recv(source = master)\n",
        "    k = 0\n",
        "    tokenized_dt = {}\n",
        "    #for every slice of data starting from the first slice, process data\n",
        "    for data in worker_data: \n",
        "        tokenized_dt[k] = txt_tokenize(text_process(data))\n",
        "        #increament k\n",
        "        k +=1 \n",
        "        \n",
        "    #after each worker has processes its corresponding slice, send it back to master\n",
        "    dt = [v for v in tokenized_dt.values()]\n",
        "    comm.send(dt,master)\n",
        "\n",
        "    \n",
        "#in case of only one process\n",
        "if rank==0:\n",
        "    t = 0\n",
        "    tokenized_dt = {}\n",
        "    for data in d_slices[0]:\n",
        "        tokenized_dt[t] = txt_tokenize(text_process(data))\n",
        "        t = t+1\n",
        "    tokens = tokens+[v for v in tokenized_dt.values()]\n",
        "    for i in range(size-1):\n",
        "        w_data = comm.recv(source = i+1)\n",
        "        tokens = tokens + w_data\n",
        "#end time\n",
        "end_time = MPI.Wtime()\n",
        "proecessing_time = end_time - start_time\n",
        "print('Processing time: {}'. format(proecessing_time))\n",
        "       \n",
        "print(\"\\nThe Number of tokenized documents:\"+str(len(tokens)))\n",
        "#print(\"Sample Tokens for :\"+str(data_array[0])+\"\\n\"+str(tokens[0]))\n",
        "#f = open(\"tokens.txt\",\"w+\")\n",
        "#f.write(str(tokens))\n",
        "#f.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIZJWe8mAcMl"
      },
      "source": [
        "## Step 2: Calculate Term Frequency (TF) \n",
        "\n",
        "Term frequency, tf(t,d), is the relative frequency of term t within document d,\n",
        "\n",
        "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/dd4f8a91dd0d28a11c00c94a13a315a5b49a8070\" width=300 height=80>\n",
        "\n",
        "\n",
        "* Where $ft,d$ is the raw count of a term in a document, i.e., the number of times that term $t$ occurs in document $d$. \n",
        "\n",
        "* Boolean \"frequencies\": tf(t,d) = 1 if $t$ occurs in $d$ and 0 otherwise;\n",
        "\n",
        "* Term frequency adjusted for document length: $tf(t,d) = ft,d ÷ (number of words in d )$\n",
        "\n",
        "* Logarithmically scaled frequency: $tf(t,d) = log (1 + ft,d)$\n",
        "* Augmented frequency, to prevent a bias towards longer documents\n",
        "\n",
        "source: [https://en.wikipedia.org/wiki/Tf%E2%80%93idf]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7S1hTM3TAcMl",
        "outputId": "88fffd2a-04c2-4e90-a85c-8a97e3e4871d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens received:11314\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:43: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  result = getattr(asarray(obj), method)(*args, **kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of documents processes:11314\n",
            "\n",
            "Sample tokens term frequencies \n",
            "[['addit', 1], ['anyon', 2], ['bodi', 1], ['bumper', 1], ['call', 1], ['car', 4], ['could', 1], ['day', 1], ['door', 2], ['e', 1], ['earli', 1], ['engin', 1], ['enlighten', 1], ['f', 1], ['front', 1], ['funki', 1], ['histori', 1], ['info', 1], ['know', 1], ['late', 1], ['look', 2], ['made', 1], ['mail', 1], ['model', 1], ['n', 1], ['name', 1], ['pleas', 1], ['product', 1], ['realli', 1], ['rest', 1], ['ricklin', 1], ['saw', 1], ['separ', 1], ['small', 1], ['spec', 1], ['sport', 1], ['tellm', 1], ['whatev', 1], ['wonder', 1], ['year', 1]]\n",
            "Processing time: 8.542964453999957\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from mpi4py import MPI\n",
        "\n",
        "#mpi routines\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.rank\n",
        "size = comm.size\n",
        "master = 0\n",
        "\n",
        "#calculate term frequency\n",
        "term_f  = {}\n",
        "#count frequency of words in documents\n",
        "def term_freq(data):\n",
        "    return ([[i, data.count(i)] for i in set(data)])\n",
        "#set starting time\n",
        "start_time = MPI.Wtime()\n",
        "\n",
        "#message passing\n",
        "if rank==0:\n",
        "    \n",
        "    \n",
        "    print('Number of tokens received:' + str(len(tokens)))\n",
        "    dt_slices = np.array_split(tokens,size)\n",
        "    \n",
        "    #send slices of the array to each workers \n",
        "    for worker in range(size-1):\n",
        "        comm.send(dt_slices[worker+1],dest=worker+1)\n",
        "    \n",
        "\n",
        "else:\n",
        "    #receove data\n",
        "    worker_dt = comm.recv(source=master)\n",
        "    #once data received, each worker will calculate term frenq\n",
        "    tf = {}\n",
        "    pointer = len(worker_dt)*rank \n",
        "    for val in worker_dt:\n",
        "        tf[pointer] = term_freq(val)\n",
        "        #move pointer for next worker\n",
        "        pointer = pointer+1\n",
        "    \n",
        "    #send resultant data back to master\n",
        "    comm.send(tf,dest= master)\n",
        "    \n",
        "#in case we have only one worker\n",
        "if rank ==0:\n",
        "    t = 0\n",
        "    tf = {}\n",
        "    for val in dt_slices[0]:\n",
        "        tf[t] = term_freq(val)\n",
        "        #increament index\n",
        "        t = t+1\n",
        "    \n",
        "    #update the intial term frequency dictionary\n",
        "    term_f.update(tf)\n",
        "    \n",
        "    for i in range(size-1):\n",
        "        w_data = comm.recv(source = i+1)\n",
        "        term_f.update(w_data)\n",
        "    for j in term_f:\n",
        "        term_f[j] = sorted(term_f[j])\n",
        "    \n",
        "    print('\\nNumber of documents processes:' +str(len(term_f)))\n",
        "    print('\\nSample tokens term frequencies \\n' +str(term_f[0]))\n",
        "    \n",
        "#end time\n",
        "end_time = MPI.Wtime()\n",
        "proecessing_time = end_time - start_time\n",
        "print('Processing time: {}'. format(proecessing_time))\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuQg2ZXtAcMn"
      },
      "source": [
        "## Step 3: Calculate Inverse Document Frequency (IDF)\n",
        "\n",
        "The inverse document frequency is a measure of how much information the word provides, i.e., if it is common or rare across all documents. It is the logarithmically scaled inverse fraction of the documents that contain the word. \n",
        "\n",
        "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/ac67bc0f76b5b8e31e842d6b7d28f8949dab7937\" width=300 height=80>\n",
        "\n",
        "* N: total number of documents in the corpus ${\\displaystyle N={|D|}}$ \n",
        "* ${\\displaystyle |\\{d\\in D:t\\in d\\}|}$ : number of documents where the term ${\\displaystyle t}$ appears. \n",
        "* If the term is not in the corpus, this will lead to a division-by-zero. It is therefore common to adjust the denominator to $$ {\\displaystyle 1+|\\{d\\in D:t\\in d\\}|}.$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TyDgk6hAcMo",
        "outputId": "a9f30a23-8bf6-4d91-be26-1abe5b06eb86"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens received:11314\n",
            "\n",
            "set token values:11314\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:43: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  result = getattr(asarray(obj), method)(*args, **kwds)\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "#mpi routines\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.rank\n",
        "size = comm.size\n",
        "master = 0\n",
        "\n",
        "#calculate inverse document frequency (IDF)\n",
        "token_values =[]\n",
        "IDF = {}\n",
        "N = 0\n",
        "\n",
        "#function to calculate idf\n",
        "def idf(data, token_values):\n",
        "    lst = []\n",
        "    for d in data:\n",
        "        count = 0\n",
        "        for s in token_values:\n",
        "            if len({d}.intersection(s)) > 0:\n",
        "                count = count+1\n",
        "        lst.append([d, math.log(N/count)])\n",
        "    return(lst)\n",
        "    \n",
        "#set starting time\n",
        "start_time = MPI.Wtime()\n",
        "\n",
        "#message passing\n",
        "if rank ==0:\n",
        "    \n",
        "    \n",
        "    print('Tokens received:'+str(len(tokens)))\n",
        "    N = len(tokens)\n",
        "    for t in tokens:\n",
        "        token_values.append(set(t))\n",
        "    print('\\nset token values:' +str(len(token_values)))\n",
        "    \n",
        "    dt_slices = np.array_split(tokens,size)\n",
        "    \n",
        "    #send slices to workers\n",
        "    for worker in range(size-1):\n",
        "        comm.send([dt_slices[worker+1], token_values], dest = worker+1)\n",
        "        \n",
        "        \n",
        "else:\n",
        "\n",
        "    N = len(tokens)\n",
        "    #receive data\n",
        "    worker_data = comm.recv(source=master)\n",
        "    #calculate idf\n",
        "    idf_ = {}\n",
        "    pointer = len(worker_data[0])*rank\n",
        "    \n",
        "    for val in worker_data[0]:\n",
        "        idf_[pointer] = idf(val,worker_data[1])\n",
        "        #move pointer\n",
        "        pointer = pointer+1\n",
        "    #send data to master\n",
        "    comm.send(idf_,dest = master)\n",
        "    \n",
        "    \n",
        "if rank == 0:\n",
        "    t= 0\n",
        "    idf_={}\n",
        "    for val in dt_slices[0]:\n",
        "        idf_[t] = idf(val,token_values)\n",
        "        #increament index\n",
        "        t = t+1\n",
        "    #update idf dictionary\n",
        "    IDF.update(idf_)\n",
        "    for i in range(size-1):\n",
        "        w_data = comm.recv(source = i+1)\n",
        "        IDF.update(w_data)\n",
        "        \n",
        "    for j in IDF:\n",
        "        IDF[j] = sorted(IDF[j])\n",
        "    print('\\nNumber if documents processes:'+str(len(IDF)))\n",
        "    print('\\nSample tokens inv document frequencies\\n' +str(IDF[0]))\n",
        "\n",
        "#end time\n",
        "end_time = MPI.Wtime()\n",
        "proecessing_time = end_time - start_time\n",
        "print('Processing time: {}'. format(proecessing_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dle_04HpAcMp"
      },
      "source": [
        "## Step 4: Calculate Term Frequency Inverse Document Frequency (TF-IDF) scores\n",
        "\n",
        "Formula:  \n",
        "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/10109d0e60cc9d50a1ea2f189bac0ac29a030a00\" width=300 height=80>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUu5TLL5AcMq"
      },
      "outputs": [],
      "source": [
        "#calculate TF-IDF of token in document\n",
        "TF_IDF ={}\n",
        "\n",
        "#set starting time\n",
        "start_time = MPI.Wtime()\n",
        "\n",
        "#message passing\n",
        "if rank ==0:\n",
        "    \n",
        "    \n",
        "    #slice both tf array and idf array into chunks to send to workers\n",
        "    tf_slices = np.array_split(term_f[0:100],size)\n",
        "    idf_slices = np.array_split(IDF[0:100],size)\n",
        "    #send slices to workers\n",
        "    for worker in range(size-1):\n",
        "        comm.send([tf_slices[worker+1],idf_slices[worker+1]], dest = worker +1)\n",
        "    \n",
        "        \n",
        "else:\n",
        "    #receive data\n",
        "    worker_data = comm.recv(source = master)\n",
        "    \n",
        "    #calculate tf-idf on slice received\n",
        "    local_tf_idf = {}\n",
        "    pointer = len(worker_data[0])*rank\n",
        "    for val in worker_data[0]:\n",
        "        local_tf_idf[pointer] = worker_data[0] * worker_data[1]\n",
        "        #move pointer\n",
        "        pointer = pointer + 1\n",
        "    #send resultant product to master\n",
        "    comm.send(local_tf_idf, dest = master)\n",
        "\n",
        "    \n",
        "#otehrwise \n",
        "if rank ==0:\n",
        "    local_tf_idf={}\n",
        "    pointer = 0\n",
        "    #multipliply every value in tf_slice with a corresponding value in idf_slice\n",
        "    for val in tf_slices[0]:\n",
        "        local_tf_idf[pointer] = tf_slices[val][1]*idf_slices[val][1]\n",
        "        #move pointer\n",
        "        pointer = pointer + 1\n",
        "    #update TF-IDF dict by adding all local_tf_idf values \n",
        "    TF_IDF.update(local_tf_idf)\n",
        "    \n",
        "    for j in range(size-1):\n",
        "        #receive form next worker\n",
        "        w_data = comm.recv(source= j+1)\n",
        "        #update TF-IDF dict by adding all worker values \n",
        "        TF_IDF.update(w_data)\n",
        "    print('\\nNumber if documents processed:' + str(len(TF_IDF)))\n",
        "    print('\\nSample tokens TF_IDF\\n' +str(TF_IDF[0]))\n",
        "    \n",
        " #end time\n",
        " end_time = MPI.Wtime()\n",
        " proecessing_time = end_time - start_time\n",
        " print('Processing time: {}'. format(proecessing_time))\n",
        "        "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Calculate TF-IDF score in a Distributed setting.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}